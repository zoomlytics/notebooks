{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zoomlytics/notebooks/blob/main/OAI_Assistants_API.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai instructor"
      ],
      "metadata": {
        "id": "CVM43uZPJD66"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wo_MTufb_EIP"
      },
      "outputs": [],
      "source": [
        "import openai\n",
        "import instructor\n",
        "from getpass import getpass\n",
        "from openai import Client\n",
        "client = Client(api_key=getpass(\"Paste your openai api key: \"))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import textwrap\n",
        "import builtins\n",
        "\n",
        "def wprint(*args, width=70, **kwargs):\n",
        "    \"\"\"\n",
        "    Custom print function that wraps text to a specified width.\n",
        "\n",
        "    Args:\n",
        "    *args: Variable length argument list.\n",
        "    width (int): The maximum width of wrapped lines.\n",
        "    **kwargs: Arbitrary keyword arguments.\n",
        "    \"\"\"\n",
        "    wrapper = textwrap.TextWrapper(width=width)\n",
        "\n",
        "    # Process all arguments to make sure they are strings and wrap them\n",
        "    wrapped_args = [wrapper.fill(str(arg)) for arg in args]\n",
        "\n",
        "    # Call the built-in print function with the wrapped text\n",
        "    builtins.print(*wrapped_args, **kwargs)"
      ],
      "metadata": {
        "id": "rFe6s1m9x0Hm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Get completion\n",
        "This function is used to get a completion from a given thread."
      ],
      "metadata": {
        "id": "udu9Oybv1Bi7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "def get_completion(message, agent, funcs, thread):\n",
        "    \"\"\"\n",
        "    Executes a thread based on a provided message and retrieves the completion result.\n",
        "\n",
        "    This function submits a message to a specified thread, triggering the execution of an array of functions\n",
        "    defined within a func parameter. Each function in the array must implement a `run()` method that returns the outputs.\n",
        "\n",
        "    Parameters:\n",
        "    - message (str): The input message to be processed.\n",
        "    - agent (OpenAI Assistant): The agent instance that will process the message.\n",
        "    - funcs (list): A list of function objects, defined with the instructor library.\n",
        "    - thread (Thread): The OpenAI Assistants API thread responsible for managing the execution flow.\n",
        "\n",
        "    Returns:\n",
        "    - str: The completion output as a string, obtained from the agent following the execution of input message and functions.\n",
        "    \"\"\"\n",
        "\n",
        "    # create new message in the thread\n",
        "    message = client.beta.threads.messages.create(\n",
        "        thread_id=thread.id,\n",
        "        role=\"user\",\n",
        "        content=message\n",
        "    )\n",
        "\n",
        "    # run this thread\n",
        "    run = client.beta.threads.runs.create(\n",
        "      thread_id=thread.id,\n",
        "      assistant_id=agent.id,\n",
        "    )\n",
        "\n",
        "    while True:\n",
        "      # wait until run completes\n",
        "      while run.status in ['queued', 'in_progress']:\n",
        "        run = client.beta.threads.runs.retrieve(\n",
        "          thread_id=thread.id,\n",
        "          run_id=run.id\n",
        "        )\n",
        "        time.sleep(1)\n",
        "\n",
        "      # function execution\n",
        "      if run.status == \"requires_action\":\n",
        "        tool_calls = run.required_action.submit_tool_outputs.tool_calls\n",
        "        tool_outputs = []\n",
        "        for tool_call in tool_calls:\n",
        "          wprint('\\033[31m' + str(tool_call.function), '\\033[0m')\n",
        "          # find the tool to be executed\n",
        "          func = next(iter([func for func in funcs if func.__name__ == tool_call.function.name]))\n",
        "\n",
        "          try:\n",
        "            # init tool\n",
        "            func = func(**eval(tool_call.function.arguments))\n",
        "            # get outputs from the tool\n",
        "            output = func.run()\n",
        "          except Exception as e:\n",
        "            output = \"Error: \" + str(e)\n",
        "\n",
        "          wprint(f\"\\033[33m{tool_call.function.name}: \", output, '\\033[0m')\n",
        "          tool_outputs.append({\"tool_call_id\": tool_call.id, \"output\": output})\n",
        "\n",
        "        # submit tool outputs\n",
        "        run = client.beta.threads.runs.submit_tool_outputs(\n",
        "            thread_id=thread.id,\n",
        "            run_id=run.id,\n",
        "            tool_outputs=tool_outputs\n",
        "        )\n",
        "      # error\n",
        "      elif run.status == \"failed\":\n",
        "        raise Exception(\"Run Failed. Error: \", run.last_error)\n",
        "      # return assistant message\n",
        "      else:\n",
        "        messages = client.beta.threads.messages.list(\n",
        "          thread_id=thread.id\n",
        "        )\n",
        "        message = messages.data[0].content[0].text.value\n",
        "        return message"
      ],
      "metadata": {
        "id": "nBxWu5H31AeO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Code Assistant\n",
        "This agent is responsible for creating and executing python files."
      ],
      "metadata": {
        "id": "lckNJJKMPHDe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List\n",
        "from pydantic import Field\n",
        "from instructor import OpenAISchema\n",
        "\n",
        "class ExecutePyFile(OpenAISchema):\n",
        "    \"\"\"Run existing python file from local disc.\"\"\"\n",
        "    file_name: str = Field(\n",
        "        ..., description=\"The path to the .py file to be executed.\"\n",
        "    )\n",
        "\n",
        "    def run(self):\n",
        "      \"\"\"Executes a Python script at the given file path and captures its output and errors.\"\"\"\n",
        "      try:\n",
        "          result = subprocess.run(\n",
        "              ['python3', self.file_name],\n",
        "              text=True,\n",
        "              capture_output=True,\n",
        "              check=True\n",
        "          )\n",
        "          return result.stdout\n",
        "      except subprocess.CalledProcessError as e:\n",
        "          return f\"An error occurred: {e.stderr}\"\n",
        "\n",
        "class File(OpenAISchema):\n",
        "    \"\"\"\n",
        "    Python file with an appropriate name, containing code that can be saved and executed locally at a later time. This environment has access to all standard Python packages and the internet.\n",
        "    \"\"\"\n",
        "    chain_of_thought: str = Field(...,\n",
        "        description=\"Think step by step to determine the correct actions that are needed to be taken in order to complete the task.\")\n",
        "    file_name: str = Field(\n",
        "        ..., description=\"The name of the file including the extension\"\n",
        "    )\n",
        "    body: str = Field(..., description=\"Correct contents of a file\")\n",
        "\n",
        "    def run(self):\n",
        "        with open(self.file_name, \"w\") as f:\n",
        "            f.write(self.body)\n",
        "\n",
        "        return \"File written to \" + self.file_name"
      ],
      "metadata": {
        "id": "fJKJqN7wM1PV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "client = OpenAI(api_key=openai.api_key)\n",
        "\n",
        "code_assistant_funcs = [File, ExecutePyFile]\n",
        "\n",
        "code_assistant = client.beta.assistants.create(\n",
        "  name='Code Assistant Agent',\n",
        "  instructions=\"As a top-tier programming AI, you are adept at creating accurate Python scripts. You will properly name files and craft precise Python code with the appropriate imports to fulfill the user's request. Ensure to execute the necessary code before responding to the user.\",\n",
        "  model=\"gpt-4-1106-preview\",\n",
        "  tools=[{\"type\": \"function\", \"function\": File.openai_schema},\n",
        "         {\"type\": \"function\", \"function\": ExecutePyFile.openai_schema},]\n",
        ")"
      ],
      "metadata": {
        "id": "16QWQ643KDI-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# User Proxy\n",
        "This agent facilitates the conversation between the user and other agents, ensuring successful completion of the task."
      ],
      "metadata": {
        "id": "zPJlj_HMPI_B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import subprocess\n",
        "from enum import Enum\n",
        "from pydantic import PrivateAttr\n",
        "from typing import Literal\n",
        "\n",
        "agents_and_threads = {\n",
        "    \"code_assistant\": {\n",
        "        \"agent\": code_assistant,\n",
        "        \"thread\": None,\n",
        "        \"funcs\": code_assistant_funcs\n",
        "    }\n",
        "}\n",
        "\n",
        "class SendMessage(OpenAISchema):\n",
        "    \"\"\"Send messages to other specialized agents in this group chat.\"\"\"\n",
        "    recepient:Literal['code_assistant'] = Field(..., description=\"code_assistant is a world class programming AI capable of executing python code.\")\n",
        "    message: str = Field(...,\n",
        "        description=\"Specify the task required for the recipient agent to complete. Focus instead on clarifying what the task entails, rather than providing detailed instructions.\")\n",
        "\n",
        "    def run(self):\n",
        "      recepient = agents_and_threads[self.recepient]\n",
        "      # if there is no thread between user proxy and this agent, create one\n",
        "      if not recepient[\"thread\"]:\n",
        "        recepient[\"thread\"] = client.beta.threads.create()\n",
        "\n",
        "      message = get_completion(message=self.message, **recepient)\n",
        "\n",
        "      return message"
      ],
      "metadata": {
        "id": "zhR4mZyaM63P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "user_proxy_tools = [SendMessage]\n",
        "\n",
        "user_proxy = client.beta.assistants.create(\n",
        "  name='User Proxy Agent',\n",
        "  instructions=\"\"\"As a user proxy agent, your responsibility is to streamline the dialogue between the user and specialized agents within this group chat.\n",
        "Your duty is to articulate user requests accurately to the relevant agents and maintain ongoing communication with them to guarantee the user's task is carried out to completion.\n",
        "Please do not respond to the user until the task is complete, an error has been reported by the relevant agent, or you are certain of your response.\"\"\",\n",
        "  model=\"gpt-4-1106-preview\",\n",
        "  tools=[\n",
        "      {\"type\": \"function\", \"function\": SendMessage.openai_schema},\n",
        "  ],\n",
        ")"
      ],
      "metadata": {
        "id": "34vFZeFbMdez"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Autogen Example Questions:\n",
        "\n",
        "\n",
        "1.   What is today's date?\n",
        "2.   Compare the year-to-date gain for META and TESLA.\n",
        "3.   Plot a chart of their stock price change YTD and save to stock_price_ytd.png.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "1zfs5PW8um5V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "thread = client.beta.threads.create()\n",
        "while True:\n",
        "  user_message = input(\"User: \")\n",
        "\n",
        "  message = get_completion(user_message, user_proxy, user_proxy_tools, thread)\n",
        "\n",
        "  wprint(f\"\\033[34m{user_proxy.name}: \", message,'\\033[0m')"
      ],
      "metadata": {
        "id": "QpdprvJP9ocZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}